"""GitHub ç›¸ä¼¼é¡¹ç›®æœç´¢å·¥å…·

æ ¹æ®ç®€å†ä¸­çš„é¡¹ç›®æè¿°ï¼Œæœç´¢ GitHub ä¸Šç›¸ä¼¼çš„é¡¹ç›®ï¼š
1. åŸºäºé¡¹ç›®æŠ€æœ¯æ ˆå’ŒèƒŒæ™¯ç”Ÿæˆç²¾å‡†æ£€ç´¢è¯
2. æ‰¾åˆ°æŠ€æœ¯æ ˆ/åœºæ™¯ç›¸è¿‘çš„å¼€æºé¡¹ç›®
3. ä»è¿™äº›é¡¹ç›®ä¸­æå–å¯å­¦ä¹ çš„æŠ€æœ¯äº®ç‚¹
4. ç”Ÿæˆç®€å†ä¼˜åŒ–å»ºè®®

æœç´¢ç»“æœä¼šè‡ªåŠ¨ç”Ÿæˆæ ¼å¼åŒ–çš„ Markdown æ–‡æ¡£ï¼Œ
Agent åº”ä½¿ç”¨ write_file ä¿å­˜åˆ° /references/ ç›®å½•ã€‚
"""
import logging
import re
from datetime import datetime
from typing import Any

import requests

from ._internal import get_document_path, get_github_headers

logger = logging.getLogger(__name__)

GITHUB_API_BASE = "https://api.github.com"

# éœ€è¦æ’é™¤çš„å®˜æ–¹æ¡†æ¶/åº“ä»“åº“
EXCLUDED_REPOS = {
    "langchain-ai/langchain", "langchain-ai/langgraph", "langchain-ai/langserve",
    "openai/openai-python", "openai/openai-node", "openai/whisper",
    "anthropics/anthropic-sdk-python", "run-llama/llama_index",
    "chroma-core/chroma", "qdrant/qdrant", "milvus-io/milvus",
    "facebook/react", "vuejs/vue", "vercel/next.js",
    "tiangolo/fastapi", "pallets/flask", "django/django",
    "pytorch/pytorch", "tensorflow/tensorflow", "huggingface/transformers",
}

EXCLUDED_ORGS = {
    "langchain-ai", "openai", "anthropics", "run-llama", "huggingface",
    "facebook", "vuejs", "vercel", "tiangolo", "pallets", "django",
    "pytorch", "tensorflow",
}

# æŠ€æœ¯æ ˆåˆ°ä¸šåŠ¡åœºæ™¯çš„æ˜ å°„
TECH_TO_SCENARIOS = {
    # AI/LLM ç›¸å…³
    "langchain": ["chatbot", "RAG", "document QA", "AI assistant", "knowledge base"],
    "langgraph": ["agent workflow", "multi-agent", "AI orchestration", "state machine"],
    "rag": ["document QA", "knowledge retrieval", "semantic search", "chat with docs"],
    "llm": ["chatbot", "text generation", "AI assistant", "content generation"],
    "openai": ["GPT integration", "AI chat", "text completion", "embeddings"],
    "embedding": ["semantic search", "similarity", "vector search", "recommendation"],
    "vector": ["similarity search", "semantic retrieval", "recommendation system"],

    # å‰ç«¯
    "react": ["dashboard", "admin panel", "web app", "SPA", "UI components"],
    "nextjs": ["full-stack app", "SSR", "static site", "e-commerce", "blog"],
    "typescript": ["type-safe", "enterprise app", "scalable frontend"],

    # åç«¯
    "fastapi": ["REST API", "microservice", "async backend", "ML serving"],
    "python": ["backend service", "data processing", "automation", "scripting"],
    "redis": ["caching", "session", "real-time", "message queue"],
    "postgresql": ["relational data", "CRUD", "analytics", "transaction"],
    "mongodb": ["document store", "flexible schema", "real-time sync"],

    # æ¶æ„
    "microservices": ["distributed system", "service mesh", "API gateway"],
    "docker": ["containerization", "deployment", "CI/CD"],
    "kubernetes": ["orchestration", "scaling", "cloud-native"],
}


def _is_framework_repo(repo: dict) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ¡†æ¶/åº“ä»“åº“"""
    full_name = repo.get("full_name", "").lower()
    owner = full_name.split("/")[0] if "/" in full_name else ""
    description = (repo.get("description") or "").lower()

    if full_name in {r.lower() for r in EXCLUDED_REPOS}:
        return True
    if owner in {o.lower() for o in EXCLUDED_ORGS}:
        return True

    framework_keywords = ["framework for", "library for", "sdk for", "official", "a framework", "a library"]
    if any(kw in description for kw in framework_keywords):
        return True

    return False


async def search_similar_projects(
    resume_item: str,
    tech_stack: list[str],
    project_type: str = "",
    max_results: int = 8,
) -> dict[str, Any]:
    """æ ¹æ®ç®€å†é¡¹ç›®æè¿°æœç´¢ç›¸ä¼¼çš„ GitHub é¡¹ç›®

    åŸºäºä½ æ­£åœ¨ä¿®æ”¹çš„ç®€å†é¡¹ç›®ï¼Œç»“åˆæŠ€æœ¯æ ˆå’Œé¡¹ç›®èƒŒæ™¯ï¼Œ
    æœç´¢ GitHub ä¸Šç›¸ä¼¼çš„é¡¹ç›®ï¼Œä»ä¸­å­¦ä¹ å¯ä»¥è¡¥å……åˆ°ç®€å†çš„æŠ€æœ¯äº®ç‚¹ã€‚

    Args:
        resume_item: ç®€å†ä¸­çš„é¡¹ç›®æè¿°ï¼ˆå¦‚ "è®¾è®¡å¹¶å®ç°äº†åŸºäº LangGraph çš„å¤š Agent åä½œç³»ç»Ÿ"ï¼‰
        tech_stack: é¡¹ç›®ä½¿ç”¨çš„æŠ€æœ¯æ ˆåˆ—è¡¨ï¼ˆå¦‚ ["LangGraph", "FastAPI", "Redis", "PostgreSQL"]ï¼‰
        project_type: é¡¹ç›®ç±»å‹ï¼ˆå¯é€‰ï¼Œå¦‚ "AI Agent", "åç«¯æœåŠ¡", "å…¨æ ˆåº”ç”¨"ï¼‰
        max_results: è¿”å›çš„æœ€å¤§é¡¹ç›®æ•°é‡ï¼ˆé»˜è®¤8ä¸ªï¼‰

    Returns:
        ç›¸ä¼¼é¡¹ç›®åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
        - similar_projects: ç›¸ä¼¼é¡¹ç›®åˆ—è¡¨ï¼ˆå« README æ‘˜è¦å’ŒæŠ€æœ¯æ ˆï¼‰
        - learnable_highlights: å¯å­¦ä¹ çš„æŠ€æœ¯äº®ç‚¹
        - enhancement_suggestions: ç®€å†ä¼˜åŒ–å»ºè®®
    """
    results = {
        "original_item": resume_item,
        "tech_stack": tech_stack,
        "project_type": project_type,
        "search_queries": [],
        "similar_projects": [],
        "learnable_highlights": [],
        "enhancement_suggestions": [],
        "keywords_to_add": [],
    }

    # 1. æ ¹æ®é¡¹ç›®ä¸Šä¸‹æ–‡ç”Ÿæˆç²¾å‡†æ£€ç´¢è¯
    search_queries = _generate_context_queries(resume_item, tech_stack, project_type)
    results["search_queries"] = search_queries

    # 2. æœç´¢ç›¸ä¼¼é¡¹ç›®
    seen_repos = set()
    for query in search_queries:
        repos = await _search_github_repos(query, max_results=max_results)

        for repo in repos:
            repo_key = repo.get("full_name", "")
            if repo_key in seen_repos:
                continue
            seen_repos.add(repo_key)

            if _is_framework_repo(repo):
                continue

            # è®¡ç®—ä¸ç”¨æˆ·é¡¹ç›®çš„ç›¸ä¼¼åº¦
            similarity = _calculate_similarity(repo, tech_stack, resume_item)
            if similarity < 0.2:  # ç›¸ä¼¼åº¦å¤ªä½çš„è·³è¿‡
                continue

            # è·å– README
            readme = await _fetch_readme(repo_key)
            readme_summary = _summarize_readme(readme) if readme else ""

            # æå–æŠ€æœ¯äº®ç‚¹
            tech_highlights = _extract_tech_highlights(repo, readme)

            results["similar_projects"].append({
                "name": repo.get("full_name", ""),
                "url": repo.get("html_url", ""),
                "description": repo.get("description", "") or "",
                "stars": repo.get("stargazers_count", 0),
                "language": repo.get("language", ""),
                "topics": repo.get("topics", []),
                "similarity_score": similarity,
                "readme_summary": readme_summary,
                "tech_highlights": tech_highlights,
                "matched_query": query,
            })

    # 3. æŒ‰ç›¸ä¼¼åº¦å’Œ stars ç»¼åˆæ’åº
    results["similar_projects"] = sorted(
        results["similar_projects"],
        key=lambda x: (x.get("similarity_score", 0) * 0.6 + min(x.get("stars", 0) / 10000, 0.4)),
        reverse=True
    )[:max_results]

    # 4. ä»ç›¸ä¼¼é¡¹ç›®ä¸­æç‚¼å¯å­¦ä¹ çš„æŠ€æœ¯äº®ç‚¹
    results["learnable_highlights"] = _extract_learnable_highlights(
        results["similar_projects"], tech_stack
    )

    # 5. ç”Ÿæˆç®€å†ä¼˜åŒ–å»ºè®®
    results["enhancement_suggestions"] = _generate_enhancement_suggestions(
        resume_item, results["similar_projects"], results["learnable_highlights"]
    )

    # 6. æå–å»ºè®®æ·»åŠ çš„å…³é”®è¯
    results["keywords_to_add"] = _extract_keywords_to_add(
        results["similar_projects"], tech_stack
    )

    # 7. ç”Ÿæˆå‚è€ƒæ–‡æ¡£
    results["_document"] = {
        "content": _format_document(results),
        "suggested_path": get_document_path("similar_projects", "_".join(tech_stack[:3])),
        "save_instruction": "è¯·ä½¿ç”¨ write_file å·¥å…·å°†æ­¤æ–‡æ¡£ä¿å­˜åˆ° suggested_path è·¯å¾„",
    }

    return results


def _generate_context_queries(resume_item: str, tech_stack: list[str], project_type: str) -> list[str]:
    """æ ¹æ®é¡¹ç›®ä¸Šä¸‹æ–‡ç”Ÿæˆç²¾å‡†æ£€ç´¢è¯"""
    queries = []

    # 1. åŸºäºæŠ€æœ¯æ ˆç»„åˆç”ŸæˆæŸ¥è¯¢
    if len(tech_stack) >= 2:
        queries.append(f"{tech_stack[0]} {tech_stack[1]} project")
        queries.append(f"{tech_stack[0]} {tech_stack[1]} application")

    # 2. åŸºäºé¡¹ç›®ç±»å‹å’ŒæŠ€æœ¯æ ˆ
    if project_type:
        queries.append(f"{project_type} {tech_stack[0]}")
    else:
        # ä»ç®€å†æè¿°ä¸­æ¨æ–­é¡¹ç›®ç±»å‹
        item_lower = resume_item.lower()
        if any(kw in item_lower for kw in ["agent", "æ™ºèƒ½ä½“", "å¤šagent", "multi-agent"]):
            queries.append(f"AI agent {tech_stack[0]}")
            queries.append("multi-agent system")
        elif any(kw in item_lower for kw in ["chatbot", "å¯¹è¯", "chat", "é—®ç­”"]):
            queries.append(f"chatbot {tech_stack[0]}")
            queries.append("conversational AI")
        elif any(kw in item_lower for kw in ["rag", "æ£€ç´¢", "çŸ¥è¯†åº“", "æ–‡æ¡£"]):
            queries.append(f"RAG {tech_stack[0]}")
            queries.append("document QA system")
        elif any(kw in item_lower for kw in ["api", "æœåŠ¡", "åç«¯", "backend"]):
            queries.append(f"API service {tech_stack[0]}")
            queries.append(f"{tech_stack[0]} microservice")

    # 3. åŸºäºæŠ€æœ¯æ ˆçš„å…¸å‹ä¸šåŠ¡åœºæ™¯
    for tech in tech_stack[:2]:
        tech_lower = tech.lower()
        if tech_lower in TECH_TO_SCENARIOS:
            scenarios = TECH_TO_SCENARIOS[tech_lower][:2]
            for scenario in scenarios:
                queries.append(f"{tech} {scenario}")

    # 4. é€šç”¨åº”ç”¨åœºæ™¯æŸ¥è¯¢
    for tech in tech_stack[:2]:
        queries.append(f"built with {tech}")
        queries.append(f"{tech} demo")

    # å»é‡å¹¶é™åˆ¶æ•°é‡
    seen = set()
    unique_queries = []
    for q in queries:
        q_lower = q.lower()
        if q_lower not in seen:
            seen.add(q_lower)
            unique_queries.append(q)

    return unique_queries[:8]


def _calculate_similarity(repo: dict, tech_stack: list[str], resume_item: str) -> float:
    """è®¡ç®—ä»“åº“ä¸ç”¨æˆ·é¡¹ç›®çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
    score = 0.0
    total_weight = 0.0

    # 1. æŠ€æœ¯æ ˆåŒ¹é…ï¼ˆæƒé‡ 0.5ï¼‰
    tech_weight = 0.5
    total_weight += tech_weight

    topics = [t.lower() for t in repo.get("topics", [])]
    description = (repo.get("description") or "").lower()
    language = (repo.get("language") or "").lower()

    matched_techs = 0
    for tech in tech_stack:
        tech_lower = tech.lower()
        if tech_lower in topics or tech_lower in description or tech_lower == language:
            matched_techs += 1

    if tech_stack:
        score += tech_weight * (matched_techs / len(tech_stack))

    # 2. æè¿°å…³é”®è¯åŒ¹é…ï¼ˆæƒé‡ 0.3ï¼‰
    desc_weight = 0.3
    total_weight += desc_weight

    resume_keywords = set(re.findall(r'[a-zA-Z]+', resume_item.lower()))
    resume_keywords.update(re.findall(r'[\u4e00-\u9fa5]+', resume_item))

    if resume_keywords and description:
        matched_keywords = sum(1 for kw in resume_keywords if kw in description)
        score += desc_weight * min(matched_keywords / 5, 1.0)

    # 3. é¡¹ç›®æ´»è·ƒåº¦ï¼ˆæƒé‡ 0.2ï¼‰
    activity_weight = 0.2
    total_weight += activity_weight

    stars = repo.get("stargazers_count", 0)
    if stars > 1000:
        score += activity_weight
    elif stars > 100:
        score += activity_weight * 0.7
    elif stars > 10:
        score += activity_weight * 0.4

    return min(score / total_weight, 1.0) if total_weight > 0 else 0.0


def _extract_tech_highlights(repo: dict, readme: str) -> list[str]:
    """ä»ä»“åº“ä¸­æå–æŠ€æœ¯äº®ç‚¹"""
    highlights = []

    # ä» topics æå–
    topics = repo.get("topics", [])
    if topics:
        highlights.extend(topics[:5])

    # ä» README ä¸­æå–åŠŸèƒ½ç‰¹æ€§
    if readme:
        feature_section = re.search(
            r'(?:##?\s*(?:Features?|åŠŸèƒ½|Highlights?|ç‰¹æ€§|Key Features?)[^\n]*\n)((?:[-*]\s+[^\n]+\n?)+)',
            readme, re.IGNORECASE
        )
        if feature_section:
            features = re.findall(r'[-*]\s+([^\n]+)', feature_section.group(1))
            highlights.extend(features[:5])

        # æå–æŠ€æœ¯å…³é”®è¯
        tech_patterns = [
            r'\b(async(?:io)?|concurrent|parallel)\b',
            r'\b(streaming|real-?time|websocket)\b',
            r'\b(cache|caching|redis|memcached)\b',
            r'\b(queue|message|kafka|rabbitmq)\b',
            r'\b(vector|embedding|semantic)\b',
            r'\b(microservice|distributed|scalable)\b',
            r'\b(CI/?CD|docker|kubernetes|k8s)\b',
            r'\b(GraphQL|REST|gRPC|API)\b',
            r'\b(authentication|OAuth|JWT)\b',
            r'\b(monitoring|logging|observability)\b',
        ]
        for pattern in tech_patterns:
            matches = re.findall(pattern, readme, re.IGNORECASE)
            highlights.extend(matches[:2])

    # å»é‡
    seen = set()
    unique = []
    for h in highlights:
        h_lower = h.lower().strip()
        if h_lower and h_lower not in seen and len(h_lower) > 2:
            seen.add(h_lower)
            unique.append(h)

    return unique[:10]


def _extract_learnable_highlights(projects: list[dict], user_tech_stack: list[str]) -> list[dict]:
    """ä»ç›¸ä¼¼é¡¹ç›®ä¸­æç‚¼å¯å­¦ä¹ çš„æŠ€æœ¯äº®ç‚¹"""
    highlights = []
    seen_highlights = set()

    user_tech_lower = {t.lower() for t in user_tech_stack}

    for project in projects[:5]:
        project_highlights = project.get("tech_highlights", [])

        for h in project_highlights:
            h_lower = h.lower()
            if h_lower in user_tech_lower:
                continue
            if h_lower in seen_highlights:
                continue

            seen_highlights.add(h_lower)
            highlights.append({
                "highlight": h,
                "source_project": project.get("name", ""),
                "source_stars": project.get("stars", 0),
            })

    return highlights[:15]


def _generate_enhancement_suggestions(
    resume_item: str,
    similar_projects: list[dict],
    learnable_highlights: list[dict]
) -> list[str]:
    """ç”Ÿæˆç®€å†ä¼˜åŒ–å»ºè®®"""
    suggestions = []

    if not similar_projects:
        return ["æœªæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„é¡¹ç›®ï¼Œå»ºè®®æ‰‹åŠ¨æœç´¢ç›¸å…³å¼€æºé¡¹ç›®ä½œä¸ºå‚è€ƒ"]

    # 1. åŸºäºé«˜æ˜Ÿé¡¹ç›®çš„é€šç”¨å»ºè®®
    top_project = similar_projects[0] if similar_projects else None
    if top_project and top_project.get("stars", 0) > 100:
        suggestions.append(
            f"å‚è€ƒé¡¹ç›® [{top_project['name']}]({top_project['url']}) (â­{top_project['stars']:,}) "
            f"çš„æè¿°æ–¹å¼ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜å…³æ³¨åº¦çš„åŒç±»é¡¹ç›®"
        )

    # 2. åŸºäºå¯å­¦ä¹ äº®ç‚¹çš„å…·ä½“å»ºè®®
    tech_keywords = [h["highlight"] for h in learnable_highlights[:5]]
    if tech_keywords:
        suggestions.append(
            f"è€ƒè™‘åœ¨æè¿°ä¸­è¡¥å……ä»¥ä¸‹æŠ€æœ¯ç»†èŠ‚ï¼š{', '.join(tech_keywords)}"
        )

    # 3. æ£€æŸ¥æ˜¯å¦ç¼ºå°‘é‡åŒ–æŒ‡æ ‡
    has_numbers = bool(re.search(r'\d+', resume_item))
    if not has_numbers:
        suggestions.append(
            "å»ºè®®æ·»åŠ é‡åŒ–æŒ‡æ ‡ï¼ˆå¦‚ï¼šå¤„ç† X æ¡æ•°æ®ã€å»¶è¿Ÿé™ä½ X%ã€æ”¯æŒ X å¹¶å‘ï¼‰"
        )

    # 4. åŸºäºç›¸ä¼¼é¡¹ç›®çš„åŠŸèƒ½å¯¹æ¯”å»ºè®®
    common_features = {}
    for project in similar_projects[:5]:
        for h in project.get("tech_highlights", []):
            h_lower = h.lower()
            common_features[h_lower] = common_features.get(h_lower, 0) + 1

    frequent_features = [f for f, count in common_features.items() if count >= 2]
    if frequent_features:
        suggestions.append(
            f"åŒç±»é¡¹ç›®å¸¸è§ç‰¹æ€§ï¼š{', '.join(frequent_features[:5])}ï¼Œ"
            "å¦‚æœä½ çš„é¡¹ç›®ä¹Ÿå®ç°äº†è¿™äº›ï¼Œå»ºè®®çªå‡ºè¯´æ˜"
        )

    return suggestions


def _extract_keywords_to_add(projects: list[dict], user_tech_stack: list[str]) -> list[str]:
    """æå–å»ºè®®æ·»åŠ åˆ°ç®€å†çš„å…³é”®è¯"""
    keyword_counts = {}
    user_tech_lower = {t.lower() for t in user_tech_stack}

    for project in projects:
        for topic in project.get("topics", []):
            topic_lower = topic.lower()
            if topic_lower not in user_tech_lower:
                keyword_counts[topic_lower] = keyword_counts.get(topic_lower, 0) + 1

        for h in project.get("tech_highlights", []):
            h_lower = h.lower()
            if h_lower not in user_tech_lower and len(h_lower) > 2:
                keyword_counts[h_lower] = keyword_counts.get(h_lower, 0) + 1

    sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)
    return [kw for kw, _ in sorted_keywords[:15]]


async def _search_github_repos(query: str, max_results: int = 10) -> list[dict]:
    """æœç´¢ GitHub ä»“åº“"""
    try:
        url = f"{GITHUB_API_BASE}/search/repositories"
        params = {
            "q": query,
            "sort": "stars",
            "order": "desc",
            "per_page": max_results,
        }

        response = requests.get(url, headers=get_github_headers(), params=params, timeout=15)

        if response.status_code == 200:
            return response.json().get("items", [])
        else:
            logger.warning(f"GitHub æœç´¢å¤±è´¥: {response.status_code} - {query}")

    except Exception as e:
        logger.error(f"GitHub æœç´¢å¼‚å¸¸ ({query}): {e}")

    return []


async def _fetch_readme(repo: str) -> str:
    """è·å–ä»“åº“ README å†…å®¹"""
    try:
        url = f"{GITHUB_API_BASE}/repos/{repo}/readme"
        headers = get_github_headers()
        headers["Accept"] = "application/vnd.github.v3.raw"

        response = requests.get(url, headers=headers, timeout=15)

        if response.status_code == 200:
            return response.text

    except Exception as e:
        logger.debug(f"README è·å–å¼‚å¸¸ ({repo}): {e}")

    return ""


def _summarize_readme(readme: str, max_length: int = 600) -> str:
    """ä» README æå–æ‘˜è¦"""
    if not readme:
        return ""

    lines = readme.split("\n")
    summary_parts = []
    in_code_block = False
    char_count = 0

    for line in lines:
        if line.strip().startswith("```"):
            in_code_block = not in_code_block
            continue
        if in_code_block:
            continue

        if re.match(r'^\s*(\[?!\[|<img|<a href="https://)', line):
            continue

        stripped = line.strip()
        if not stripped:
            if summary_parts and summary_parts[-1]:
                summary_parts.append("")
            continue

        cleaned = stripped
        cleaned = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', cleaned)
        cleaned = re.sub(r'`([^`]+)`', r'\1', cleaned)
        cleaned = re.sub(r'\*\*([^*]+)\*\*', r'\1', cleaned)
        cleaned = re.sub(r'\*([^*]+)\*', r'\1', cleaned)

        if cleaned.startswith("#"):
            cleaned = cleaned.lstrip("#").strip()
            if cleaned:
                cleaned = f"ã€{cleaned}ã€‘"

        summary_parts.append(cleaned)
        char_count += len(cleaned)

        if char_count >= max_length:
            break

    summary = "\n".join(summary_parts).strip()
    if len(summary) > max_length:
        summary = summary[:max_length] + "..."

    return summary


def _format_document(results: dict) -> str:
    """ç”Ÿæˆæ ¼å¼åŒ–çš„ Markdown æ–‡æ¡£"""
    lines = [
        "# ç›¸ä¼¼é¡¹ç›®åˆ†ææŠ¥å‘Š",
        "",
        f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "",
        "## ğŸ“‹ åŸå§‹ç®€å†é¡¹ç›®",
        "",
        f"> {results.get('original_item', '')}",
        "",
        f"**æŠ€æœ¯æ ˆ**: {', '.join(results.get('tech_stack', []))}",
        "",
    ]

    if results.get("project_type"):
        lines.append(f"**é¡¹ç›®ç±»å‹**: {results['project_type']}")
        lines.append("")

    # æ£€ç´¢ç­–ç•¥
    queries = results.get("search_queries", [])
    if queries:
        lines.append("## ğŸ” æ£€ç´¢ç­–ç•¥")
        lines.append("")
        lines.append("åŸºäºé¡¹ç›®ä¸Šä¸‹æ–‡ç”Ÿæˆçš„æ£€ç´¢è¯ï¼š")
        for q in queries[:6]:
            lines.append(f"- `{q}`")
        lines.append("")

    # ç›¸ä¼¼é¡¹ç›®
    projects = results.get("similar_projects", [])
    if projects:
        lines.append("## ğŸ“¦ ç›¸ä¼¼é¡¹ç›®")
        lines.append("")

        for i, project in enumerate(projects[:8], 1):
            name = project.get("name", "")
            url = project.get("url", "")
            stars = project.get("stars", 0)
            similarity = project.get("similarity_score", 0)
            description = project.get("description", "")[:120]

            lines.append(f"### {i}. [{name}]({url})")
            lines.append(f"â­ {stars:,} | ç›¸ä¼¼åº¦: {similarity:.0%}")
            if description:
                lines.append(f"> {description}")
            lines.append("")

            readme_summary = project.get("readme_summary", "")
            if readme_summary:
                lines.append("**é¡¹ç›®ç®€ä»‹:**")
                for p in readme_summary.split("\n\n")[:2]:
                    if p.strip():
                        lines.append(f"> {p.strip()}")
                lines.append("")

            highlights = project.get("tech_highlights", [])
            if highlights:
                lines.append(f"**æŠ€æœ¯äº®ç‚¹:** {', '.join(highlights[:8])}")
                lines.append("")

    # å¯å­¦ä¹ çš„æŠ€æœ¯äº®ç‚¹
    learnable = results.get("learnable_highlights", [])
    if learnable:
        lines.append("## ğŸ’¡ å¯å­¦ä¹ çš„æŠ€æœ¯äº®ç‚¹")
        lines.append("")
        lines.append("ä»ç›¸ä¼¼é¡¹ç›®ä¸­å‘ç°çš„ã€ä½ å¯èƒ½å¯ä»¥è¡¥å……åˆ°ç®€å†çš„æŠ€æœ¯ç‚¹ï¼š")
        lines.append("")
        for h in learnable[:10]:
            lines.append(f"- **{h['highlight']}** (æ¥è‡ª {h['source_project']})")
        lines.append("")

    # ç®€å†ä¼˜åŒ–å»ºè®®
    suggestions = results.get("enhancement_suggestions", [])
    if suggestions:
        lines.append("## âœ¨ ç®€å†ä¼˜åŒ–å»ºè®®")
        lines.append("")
        for i, s in enumerate(suggestions, 1):
            lines.append(f"{i}. {s}")
        lines.append("")

    # å»ºè®®æ·»åŠ çš„å…³é”®è¯
    keywords = results.get("keywords_to_add", [])
    if keywords:
        lines.append("## ğŸ·ï¸ å»ºè®®æ·»åŠ çš„å…³é”®è¯")
        lines.append("")
        lines.append("è¿™äº›å…³é”®è¯åœ¨ç›¸ä¼¼é¡¹ç›®ä¸­å‡ºç°é¢‘ç‡è¾ƒé«˜ï¼Œè€ƒè™‘åœ¨ç®€å†ä¸­ä½¿ç”¨ï¼š")
        lines.append("")
        lines.append("```")
        lines.append(", ".join(keywords[:15]))
        lines.append("```")
        lines.append("")

    return "\n".join(lines)
