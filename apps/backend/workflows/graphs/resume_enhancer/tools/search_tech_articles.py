"""æŠ€æœ¯å†…å®¹/è®¨è®ºæœç´¢å·¥å…·

æœç´¢æŠ€æœ¯ç›¸å…³çš„æ–‡ç« ã€è®¨è®ºå’Œæ¨¡å‹ï¼š
- DEV.to: è‹±æ–‡æŠ€æœ¯åšå®¢
- æ˜é‡‘: ä¸­æ–‡æŠ€æœ¯ç¤¾åŒº
- InfoQ: æ¶æ„/ä¼ä¸šçº§æ–‡ç« 
- Reddit: æŠ€æœ¯è®¨è®ºå¸–å­
- HuggingFace: AI æ¨¡å‹å’Œè®ºæ–‡

è¿”å›ç®€æ´æ‘˜è¦ + è¯¦ç»†æ–‡æ¡£å†…å®¹ï¼ˆä¾› Agent ä½¿ç”¨ write_file ä¿å­˜ï¼‰ã€‚
"""
import logging
from datetime import datetime
from typing import Any

import requests

from ._internal import get_document_path

logger = logging.getLogger(__name__)


async def search_tech_articles(
    keywords: list[str],
    language: str = "zh",
    max_results: int = 5,
) -> dict[str, Any]:
    """æœç´¢æŠ€æœ¯æ–‡ç« å’Œè®¨è®º

    ä»å¤šä¸ªæŠ€æœ¯ç¤¾åŒºæœç´¢ç›¸å…³æ–‡ç« å’Œè®¨è®ºï¼Œè·å–æœ€ä½³å®è·µå’ŒæŠ€æœ¯è§è§£ã€‚

    Args:
        keywords: æŠ€æœ¯å…³é”®è¯åˆ—è¡¨ (å¦‚ ["LLM memory", "RAG architecture"])
        language: è¯­è¨€åå¥½ ("zh" ä¸­æ–‡ä¼˜å…ˆ, "en" è‹±æ–‡ä¼˜å…ˆ)
        max_results: æ¯ä¸ªæ¥æºçš„æœ€å¤§è¿”å›ç»“æœæ•°

    Returns:
        åŒ…å«ç®€æ´æ‘˜è¦å’Œæ–‡æ¡£å†…å®¹çš„å­—å…¸ï¼š
        - summary: ç®€æ´çš„æœç´¢ç»“æœæ‘˜è¦
        - document_content: è¯¦ç»†çš„ Markdown æ–‡æ¡£å†…å®¹
        - suggested_path: å»ºè®®ä¿å­˜è·¯å¾„
    """
    results = {
        "articles": [],
        "discussions": [],
        "models": [],
        "data_sources": [],
    }

    # 1. DEV.to (è‹±æ–‡æŠ€æœ¯åšå®¢)
    dev_articles = await _search_devto(keywords, max_results)
    if dev_articles:
        results["data_sources"].append("DEV.to")
        results["articles"].extend(dev_articles)

    # 2. æ˜é‡‘ (ä¸­æ–‡æŠ€æœ¯ç¤¾åŒº)
    if language == "zh":
        juejin_articles = await _search_juejin(keywords, max_results)
        if juejin_articles:
            results["data_sources"].append("æ˜é‡‘")
            results["articles"].extend(juejin_articles)

    # 3. InfoQ (æ¶æ„/ä¼ä¸šçº§)
    if language == "zh":
        infoq_articles = await _search_infoq(keywords, max_results)
        if infoq_articles:
            results["data_sources"].append("InfoQ")
            results["articles"].extend(infoq_articles)

    # 4. Reddit (æŠ€æœ¯è®¨è®º)
    reddit_posts = await _search_reddit(keywords, max_results)
    if reddit_posts:
        results["data_sources"].append("Reddit")
        results["discussions"].extend(reddit_posts)

    # 5. HuggingFace (AI æ¨¡å‹)
    hf_models = await _search_huggingface(keywords, max_results)
    if hf_models:
        results["data_sources"].append("HuggingFace")
        results["models"].extend(hf_models)

    # ç”Ÿæˆæ–‡æ¡£å†…å®¹å’Œç®€æ´æ‘˜è¦
    document_content = _format_document(keywords, results)
    suggested_path = get_document_path("tech_articles", "_".join(keywords[:3]))
    summary = _format_summary(keywords, results, suggested_path)

    return {
        "summary": summary,
        "document_content": document_content,
        "suggested_path": suggested_path,
    }


async def _search_devto(keywords: list[str], max_results: int = 5) -> list[dict]:
    """æœç´¢ DEV.to æ–‡ç« """
    articles = []
    try:
        for keyword in keywords[:2]:
            url = "https://dev.to/api/articles"
            params = {
                "tag": keyword.lower().replace(" ", ""),
                "per_page": max_results,
                "top": 7,
            }
            headers = {"User-Agent": "ResumeAgent/1.0"}

            response = requests.get(url, params=params, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                for item in data[:max_results]:
                    articles.append({
                        "title": item.get("title", ""),
                        "url": item.get("url", ""),
                        "summary": item.get("description", "")[:200],
                        "tags": item.get("tag_list", []),
                        "source": "DEV.to",
                        "author": item.get("user", {}).get("name", ""),
                        "reactions": item.get("positive_reactions_count", 0),
                        "published_at": item.get("published_at", ""),
                    })

            if len(articles) >= max_results:
                break

        # å¦‚æœ tag æœç´¢æ²¡æœ‰ç»“æœï¼Œå°è¯•é€šç”¨æœç´¢
        if not articles:
            url = "https://dev.to/api/articles"
            params = {"per_page": max_results}
            response = requests.get(url, params=params, headers={"User-Agent": "ResumeAgent/1.0"}, timeout=10)
            if response.status_code == 200:
                data = response.json()
                for item in data[:max_results]:
                    title = item.get("title", "").lower()
                    desc = item.get("description", "").lower()
                    if any(kw.lower() in title or kw.lower() in desc for kw in keywords):
                        articles.append({
                            "title": item.get("title", ""),
                            "url": item.get("url", ""),
                            "summary": item.get("description", "")[:200],
                            "tags": item.get("tag_list", []),
                            "source": "DEV.to",
                            "author": item.get("user", {}).get("name", ""),
                            "reactions": item.get("positive_reactions_count", 0),
                            "published_at": item.get("published_at", ""),
                        })

    except Exception as e:
        logger.warning(f"DEV.to æœç´¢å¤±è´¥: {e}")

    return articles[:max_results]


async def _search_juejin(keywords: list[str], max_results: int = 5) -> list[dict]:
    """æœç´¢æ˜é‡‘æ–‡ç« """
    articles = []
    try:
        search_query = " ".join(keywords)
        url = "https://api.juejin.cn/search_api/v1/search"
        payload = {
            "cursor": "0",
            "key_word": search_query,
            "id_type": 0,
            "limit": max_results,
            "search_type": 2,
            "sort_type": 0,
        }
        headers = {
            "Content-Type": "application/json",
            "User-Agent": "ResumeAgent/1.0",
        }

        response = requests.post(url, json=payload, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            items = data.get("data", [])
            for item in items[:max_results]:
                result_model = item.get("result_model", {})
                article_info = result_model.get("article_info", {})
                author_info = result_model.get("author_user_info", {})

                if article_info:
                    articles.append({
                        "title": article_info.get("title", ""),
                        "url": f"https://juejin.cn/post/{article_info.get('article_id', '')}",
                        "summary": article_info.get("brief_content", "")[:200],
                        "tags": [tag.get("tag_name", "") for tag in result_model.get("tags", [])],
                        "source": "æ˜é‡‘",
                        "author": author_info.get("user_name", ""),
                        "reactions": article_info.get("digg_count", 0),
                        "published_at": "",
                    })

    except Exception as e:
        logger.warning(f"æ˜é‡‘æœç´¢å¤±è´¥: {e}")

    return articles[:max_results]


async def _search_infoq(keywords: list[str], max_results: int = 3) -> list[dict]:
    """æœç´¢ InfoQ ä¸­æ–‡ç«™æ–‡ç« """
    articles = []
    try:
        url = "https://www.infoq.cn/public/v1/article/getList"
        payload = {
            "type": 1,
            "size": max_results,
            "score": None,
            "id": None,
        }
        headers = {
            "Content-Type": "application/json",
            "User-Agent": "ResumeAgent/1.0",
            "Referer": "https://www.infoq.cn/",
        }

        response = requests.post(url, json=payload, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            items = data.get("data", [])
            for item in items[:max_results]:
                title = item.get("article_title", "").lower()
                if any(kw.lower() in title for kw in keywords):
                    articles.append({
                        "title": item.get("article_title", ""),
                        "url": f"https://www.infoq.cn/article/{item.get('uuid', '')}",
                        "summary": item.get("article_summary", "")[:200],
                        "tags": [],
                        "source": "InfoQ",
                        "author": item.get("author", [{}])[0].get("nickname", "") if item.get("author") else "",
                        "reactions": item.get("views", 0),
                        "published_at": item.get("publish_time", ""),
                    })

    except Exception as e:
        logger.warning(f"InfoQ æœç´¢å¤±è´¥: {e}")

    return articles[:max_results]


async def _search_reddit(keywords: list[str], max_results: int = 5) -> list[dict]:
    """æœç´¢ Reddit è®¨è®º"""
    posts = []
    try:
        for keyword in keywords[:2]:
            # é’ˆå¯¹ AI ç›¸å…³å…³é”®è¯ä¼˜åŒ–æœç´¢
            search_term = keyword
            if "agent" in keyword.lower():
                search_term = "AI Agent"
            elif "memory" in keyword.lower() or "è®°å¿†" in keyword.lower():
                search_term = "LLM memory"

            url = "https://www.reddit.com/search.json"
            params = {
                "q": search_term,
                "sort": "relevance",
                "limit": max_results,
                "restrict_sr": False,
            }
            headers = {"User-Agent": "ResumeAgent/1.0"}

            response = requests.get(url, params=params, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                for child in data.get("data", {}).get("children", [])[:max_results]:
                    post = child.get("data", {})
                    posts.append({
                        "title": post.get("title", ""),
                        "subreddit": post.get("subreddit", ""),
                        "score": post.get("score", 0),
                        "num_comments": post.get("num_comments", 0),
                        "url": f"https://reddit.com{post.get('permalink', '')}",
                        "source": "Reddit",
                    })

    except Exception as e:
        logger.warning(f"Reddit æœç´¢å¤±è´¥: {e}")

    return posts[:max_results]


async def _search_huggingface(keywords: list[str], max_results: int = 5) -> list[dict]:
    """æœç´¢ HuggingFace æ¨¡å‹"""
    models = []
    try:
        for keyword in keywords[:2]:
            url = "https://huggingface.co/api/models"
            params = {
                "search": keyword,
                "sort": "likes",
                "direction": -1,
                "limit": max_results
            }
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                for m in data[:max_results]:
                    models.append({
                        "model_id": m.get("modelId", ""),
                        "likes": m.get("likes", 0),
                        "downloads": m.get("downloads", 0),
                        "task": m.get("pipeline_tag", ""),
                        "url": f"https://huggingface.co/{m.get('modelId', '')}",
                        "source": "HuggingFace",
                    })

    except Exception as e:
        logger.warning(f"HuggingFace æœç´¢å¤±è´¥: {e}")

    return models[:max_results]


def _format_summary(keywords: list[str], results: dict, suggested_path: str) -> str:
    """ç”Ÿæˆç®€æ´çš„æœç´¢ç»“æœæ‘˜è¦"""
    articles = results.get("articles", [])
    discussions = results.get("discussions", [])
    models = results.get("models", [])
    sources = results.get("data_sources", [])

    lines = []

    # ç»Ÿè®¡
    lines.append(f"æœç´¢å…³é”®è¯ï¼š{', '.join(keywords)}")
    lines.append(f"æ•°æ®æ¥æºï¼š{', '.join(sources)}")
    lines.append(f"å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ã€{len(discussions)} æ¡è®¨è®ºã€{len(models)} ä¸ªæ¨¡å‹")
    lines.append("")

    # Top æ–‡ç« 
    if articles:
        lines.append("**çƒ­é—¨æ–‡ç« ï¼š**")
        for article in articles[:3]:
            title = article.get("title", "")[:50]
            source = article.get("source", "")
            reactions = article.get("reactions", 0)
            lines.append(f"- [{source}] {title}... (ğŸ‘{reactions})")
        lines.append("")

    # Top æ¨¡å‹
    if models:
        lines.append("**ç›¸å…³æ¨¡å‹ï¼š**")
        for m in models[:3]:
            model_id = m.get("model_id", "")
            likes = m.get("likes", 0)
            lines.append(f"- {model_id} (â¤ï¸{likes})")
        lines.append("")

    # ä¿å­˜æç¤º
    lines.append(f"è¯·ä½¿ç”¨ write_file å°†è¯¦ç»†æŠ¥å‘Šä¿å­˜åˆ° `{suggested_path}`")

    return "\n".join(lines)


def _format_document(keywords: list[str], results: dict) -> str:
    """ç”Ÿæˆæ ¼å¼åŒ–çš„ Markdown æ–‡æ¡£"""
    lines = [
        f"# æŠ€æœ¯å†…å®¹æœç´¢æŠ¥å‘Š",
        f"",
        f"**æŸ¥è¯¢å…³é”®è¯**: {', '.join(keywords)}",
        f"**æ•°æ®æ¥æº**: {', '.join(results.get('data_sources', []))}",
        f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        f"",
    ]

    # æŠ€æœ¯æ–‡ç« 
    articles = results.get("articles", [])
    if articles:
        lines.append("## ğŸ“ æŠ€æœ¯æ–‡ç« ")
        lines.append("")

        # æŒ‰æ¥æºåˆ†ç»„
        by_source = {}
        for article in articles:
            source = article.get("source", "å…¶ä»–")
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(article)

        for source, source_articles in by_source.items():
            lines.append(f"### {source}")
            lines.append("")
            for article in source_articles:
                title = article.get("title", "æ— æ ‡é¢˜")
                url = article.get("url", "")
                summary = article.get("summary", "")
                author = article.get("author", "")
                reactions = article.get("reactions", 0)
                tags = article.get("tags", [])

                lines.append(f"**[{title}]({url})**")
                if author:
                    lines.append(f"*ä½œè€…ï¼š{author}* | ğŸ‘ {reactions}")
                if summary:
                    lines.append(f"> {summary[:150]}...")
                if tags:
                    lines.append(f"æ ‡ç­¾ï¼š{', '.join(tags[:5])}")
                lines.append("")

    # Reddit è®¨è®º
    discussions = results.get("discussions", [])
    if discussions:
        lines.append("## ğŸ’¬ Reddit è®¨è®º")
        lines.append("")
        for post in discussions:
            title = post.get("title", "")
            url = post.get("url", "")
            subreddit = post.get("subreddit", "")
            score = post.get("score", 0)
            comments = post.get("num_comments", 0)
            lines.append(f"- [{title[:80]}...]({url})")
            lines.append(f"  - r/{subreddit} | ğŸ‘ {score} | ğŸ’¬ {comments} è¯„è®º")
        lines.append("")

    # HuggingFace æ¨¡å‹
    models = results.get("models", [])
    if models:
        lines.append("## ğŸ¤— HuggingFace æ¨¡å‹")
        lines.append("")
        lines.append("| æ¨¡å‹ | ä»»åŠ¡ | Likes | Downloads |")
        lines.append("|------|------|-------|-----------|")
        for m in models:
            model_id = m.get("model_id", "")
            url = m.get("url", "")
            task = m.get("task", "-")
            likes = m.get("likes", 0)
            downloads = m.get("downloads", 0)
            lines.append(f"| [{model_id}]({url}) | {task} | â¤ï¸ {likes:,} | ğŸ“¥ {downloads:,} |")
        lines.append("")

    # ç®€å†å…³é”®è¯å»ºè®®
    lines.append("## ğŸ’¡ ç®€å†å…³é”®è¯å»ºè®®")
    lines.append("")
    lines.append("ä»ä»¥ä¸Šå†…å®¹ä¸­å¯æå–çš„æŠ€æœ¯å…³é”®è¯ï¼š")
    all_tags = []
    for article in articles:
        all_tags.extend(article.get("tags", []))
    unique_tags = list(set(all_tags))[:10]
    if unique_tags:
        lines.append(f"- {', '.join(unique_tags)}")
    else:
        lines.append(f"- {', '.join(keywords)}")
    lines.append("")

    return "\n".join(lines)
