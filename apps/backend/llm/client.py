"""LLM å®¢æˆ·ç«¯å°è£…

æä¾›ç»Ÿä¸€çš„ OpenAI å…¼å®¹æ¥å£è°ƒç”¨ã€‚
"""
from __future__ import annotations

import asyncio
import logging
import time
from typing import Any, Type, TypeVar

from openai import AsyncOpenAI
from pydantic import BaseModel

from config.app_config import config
from .config import get_model_by_type

logger = logging.getLogger(__name__)

T = TypeVar('T', bound=BaseModel)

# å…¨å±€ OpenAI å®¢æˆ·ç«¯
_openai_client: AsyncOpenAI | None = None

# å¹¶å‘æ§åˆ¶
_llm_semaphore: asyncio.Semaphore | None = None
_max_concurrent_llm: int = 50


def init_openai_client():
    """åˆå§‹åŒ–å…¨å±€ OpenAI å®¢æˆ·ç«¯"""
    global _openai_client
    if _openai_client is None:
        import httpx

        http_client = httpx.AsyncClient(
            limits=httpx.Limits(
                max_connections=100,
                max_keepalive_connections=50
            ),
            timeout=httpx.Timeout(
                connect=60.0,
                read=300.0,
                write=30.0,
                pool=120.0
            ),
        )
        _openai_client = AsyncOpenAI(
            base_url=config.openai_api_base,
            api_key=config.openai_api_key,
            http_client=http_client,
            max_retries=2
        )
        logger.info("âœ… OpenAI å®¢æˆ·ç«¯å·²åˆå§‹åŒ–")


def _get_openai_client() -> AsyncOpenAI:
    """è·å–å…¨å±€ OpenAI å®¢æˆ·ç«¯"""
    if _openai_client is None:
        raise RuntimeError(
            "OpenAI å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_openai_client()"
        )
    return _openai_client


def _get_semaphore() -> asyncio.Semaphore:
    """è·å–å¹¶å‘æ§åˆ¶ä¿¡å·é‡"""
    global _llm_semaphore
    if _llm_semaphore is None:
        _llm_semaphore = asyncio.Semaphore(_max_concurrent_llm)
    return _llm_semaphore


async def llm_request(
    messages: list[dict[str, Any]],
    model: str = "general_model",
    temperature: float = 0.7,
    timeout: int = 300
) -> str | None:
    """å‘é€ LLM è¯·æ±‚ï¼Œè¿”å›æ–‡æœ¬å†…å®¹

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        model: æ¨¡å‹ç±»å‹æˆ–å…·ä½“æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
    """
    model = get_model_by_type(model)

    async with _get_semaphore():
        start_time = time.time()
        client = _get_openai_client()

        logger.info(f"ğŸš€ LLM è¯·æ±‚: model={model}, temperature={temperature}")

        resp = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            timeout=timeout
        )

        elapsed_time = time.time() - start_time
        logger.info(f"âœ… {model} å“åº”æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")

        return resp.choices[0].message.content


async def llm_request_structured(
    messages: list[dict[str, Any]],
    response_format: Type[T],
    model: str = "general_model",
    temperature: float = 0.7,
    timeout: int = 300
) -> T | None:
    """å‘é€ç»“æ„åŒ– LLM è¯·æ±‚ï¼Œè¿”å› Pydantic å¯¹è±¡

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        response_format: Pydantic æ¨¡å‹ç±»
        model: æ¨¡å‹ç±»å‹æˆ–å…·ä½“æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        Pydantic å¯¹è±¡å®ä¾‹
    """
    import json

    model = get_model_by_type(model)

    async with _get_semaphore():
        start_time = time.time()
        client = _get_openai_client()

        logger.info(f"ğŸš€ ç»“æ„åŒ–è¯·æ±‚: model={model}, format={response_format.__name__}")

        # æ·»åŠ  JSON è¾“å‡ºæŒ‡ä»¤
        system_msg = messages[0] if messages and messages[0].get("role") == "system" else None
        if system_msg:
            system_msg["content"] += "\n\nè¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœã€‚"
        else:
            messages.insert(0, {
                "role": "system",
                "content": "è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœã€‚"
            })

        resp = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            timeout=timeout
        )

        elapsed_time = time.time() - start_time
        logger.info(f"âœ… {model} ç»“æ„åŒ–å“åº”æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")

        content = resp.choices[0].message.content
        if not content:
            return None

        # æ¸…ç† Markdown ä»£ç å—
        content = content.strip()
        if content.startswith("```json"):
            content = content[7:]
        elif content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        content = content.strip()

        # è§£æ JSON å¹¶æ„å»º Pydantic å¯¹è±¡
        data = json.loads(content)
        return response_format.model_validate(data)
